{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5e4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM predictor...\n",
      "Epoch 1/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 1.0608e-06\n",
      "Epoch 2/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.7226e-07 - val_loss: 5.9588e-07\n",
      "Epoch 3/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.5170e-07 - val_loss: 6.8514e-07\n",
      "Epoch 4/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.4127e-07 - val_loss: 1.0097e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2006e-07 - val_loss: 7.2383e-07\n",
      "Epoch 6/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.0344e-07 - val_loss: 6.1451e-07\n",
      "Epoch 7/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9597e-07 - val_loss: 8.3058e-07\n",
      "Epoch 8/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.1424e-07 - val_loss: 1.4636e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9916e-07 - val_loss: 4.9841e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.5360e-07 - val_loss: 1.8979e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.8771e-07 - val_loss: 2.6916e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.7654e-07 - val_loss: 2.0359e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.1951e-07 - val_loss: 1.3315e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.2074e-06 - val_loss: 1.6419e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6046e-07 - val_loss: 6.0343e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8165e-06 - val_loss: 2.6145e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.2430e-06 - val_loss: 8.8572e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.3740e-07 - val_loss: 2.7109e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8445e-06 - val_loss: 8.0508e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2090e-06 - val_loss: 7.5425e-06\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "LSTM scaled MSE on test: 0.000020\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training PPO agent...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 608      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    237\u001b[39m model = PPO(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env, verbose=\u001b[32m1\u001b[39m, seed=RANDOM_SEED)\n\u001b[32m    238\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining PPO agent...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPPO_TIMESTEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# -------------- Test the learned policy --------------\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Testing learned policy for a few episodes ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:116\u001b[39m, in \u001b[36mGymV26CompatibilityV0.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m \u001b[33;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgym_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 223\u001b[39m, in \u001b[36mV2VWithPredictorEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    220\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m.t >= \u001b[38;5;28mself\u001b[39m.max_t\n\u001b[32m    221\u001b[39m truncated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m info = {\u001b[33m'\u001b[39m\u001b[33mperception_gain\u001b[39m\u001b[33m'\u001b[39m: perception_gain, \u001b[33m'\u001b[39m\u001b[33mdelay\u001b[39m\u001b[33m'\u001b[39m: delay}\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obs, \u001b[38;5;28mfloat\u001b[39m(reward), terminated, truncated, info\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mV2VWithPredictorEnv._get_obs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cov_ids:\n\u001b[32m    197\u001b[39m     raw_seq = \u001b[38;5;28mself\u001b[39m.traces[cid][\u001b[38;5;28mself\u001b[39m.t - \u001b[38;5;28mself\u001b[39m.seq_len: \u001b[38;5;28mself\u001b[39m.t]\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     p = \u001b[43mpredict_next_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     predicted.append(p)\n\u001b[32m    200\u001b[39m delays = \u001b[38;5;28mself\u001b[39m.base_delays.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mpredict_next_pos\u001b[39m\u001b[34m(raw_seq)\u001b[39m\n\u001b[32m    113\u001b[39m x = np.array(raw_seq).reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m    114\u001b[39m x_scaled = scaler.transform(x).reshape(\u001b[32m1\u001b[39m, SEQ_LEN, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m p_scaled = \u001b[43mlstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m]\n\u001b[32m    116\u001b[39m p = scaler.inverse_transform(np.array([[p_scaled]]))[\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m]\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:563\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    561\u001b[39m outputs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:742\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:125\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[32m    120\u001b[39m             step,\n\u001b[32m    121\u001b[39m             step + \u001b[38;5;28mself\u001b[39m.steps_per_execution - \u001b[32m1\u001b[39m,\n\u001b[32m    122\u001b[39m             \u001b[38;5;28mself\u001b[39m._current_iterator,\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._steps_seen >= \u001b[38;5;28mself\u001b[39m._num_batches:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    707\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:731\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28mself\u001b[39m._flat_output_shapes = structure.get_flat_tensor_shapes(\n\u001b[32m    728\u001b[39m     \u001b[38;5;28mself\u001b[39m._element_spec)\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(ds_variant):\n\u001b[32m    730\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource = (\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m       \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43manonymous_iterator_v3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m          \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m          \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    734\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    735\u001b[39m     \u001b[38;5;66;03m# Add full type information to the graph so host memory types inside\u001b[39;00m\n\u001b[32m    736\u001b[39m     \u001b[38;5;66;03m# variants stay on CPU, e.g, ragged string tensors.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    740\u001b[39m     \u001b[38;5;66;03m# type inference (esp. cross-function type inference) instead of\u001b[39;00m\n\u001b[32m    741\u001b[39m     \u001b[38;5;66;03m# setting the full type information manually.\u001b[39;00m\n\u001b[32m    742\u001b[39m     fulltype = type_utils.iterator_full_type_from_spec(\n\u001b[32m    743\u001b[39m         \u001b[38;5;28mself\u001b[39m._element_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prane\\Documents\\GitHub\\intro-to-git-PraneethUday\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:204\u001b[39m, in \u001b[36manonymous_iterator_v3\u001b[39m\u001b[34m(output_types, output_shapes, name)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    203\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnonymousIteratorV3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_types\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_shapes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    208\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combined pipeline (fixed Gym API):\n",
    "- Train a shared LSTM mobility predictor (Member E)\n",
    "- Use LSTM in Gym env to provide predicted CoV positions to DRL state (Member F)\n",
    "- Train PPO agent that chooses which CoV's data to request each step\n",
    "\n",
    "Install prerequisites first (one-time):\n",
    "pip install scikit-learn tensorflow stable-baselines3 gym\n",
    "# In Jupyter/Colab prefix with `!`\n",
    "\"\"\"\n",
    "\n",
    "# -------------- Imports --------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# -------------- Config --------------\n",
    "SEQ_LEN = 8          # lookback window length for LSTM\n",
    "NUM_COV = 5          # number of candidate CoVs\n",
    "TOTAL_TIMESTEPS = 4000  # timesteps for synthetic traces\n",
    "LSTM_EPOCHS = 20\n",
    "PPO_TIMESTEPS = 8000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# -------------- Synthetic SUMO-like trace generator --------------\n",
    "def generate_synthetic_traces(num_vehicles, total_steps):\n",
    "    \"\"\"\n",
    "    Returns dict: {veh_id: positions_array_of_length_total_steps}\n",
    "    Simple motion: constant velocity with small sinusoidal + noise\n",
    "    \"\"\"\n",
    "    traces = {}\n",
    "    for vid in range(num_vehicles):\n",
    "        base_speed = np.random.uniform(0.2, 2.0)  # units per step\n",
    "        phase = np.random.uniform(0, 2*np.pi)\n",
    "        noise = np.random.normal(0, 0.05, size=total_steps)\n",
    "        pos = np.cumsum(base_speed + 0.3*np.sin(np.linspace(0, 4*np.pi, total_steps) + phase) + noise)\n",
    "        pos = pos + np.random.uniform(-50, 50)\n",
    "        traces[f\"veh_{vid}\"] = pos\n",
    "    return traces\n",
    "\n",
    "# -------------- Prepare data for LSTM (shared model) --------------\n",
    "def build_dataset_from_traces(traces, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    for veh, pos_arr in traces.items():\n",
    "        for i in range(len(pos_arr) - seq_len - 1):\n",
    "            seq = pos_arr[i:i+seq_len]\n",
    "            target = pos_arr[i+seq_len]\n",
    "            X.append(seq)\n",
    "            y.append(target)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Generate traces: 1 VU + NUM_COV CoVs\n",
    "num_vehicles = 1 + NUM_COV\n",
    "traces = generate_synthetic_traces(num_vehicles, TOTAL_TIMESTEPS)\n",
    "veh_ids = list(traces.keys())\n",
    "vu_id = veh_ids[0]\n",
    "cov_ids = veh_ids[1:NUM_COV+1]\n",
    "\n",
    "X_raw, y_raw = build_dataset_from_traces(traces, SEQ_LEN)\n",
    "\n",
    "# scale positions (shared scaler)\n",
    "scaler = MinMaxScaler()\n",
    "all_positions = np.concatenate([traces[k] for k in traces])\n",
    "scaler.fit(all_positions.reshape(-1,1))\n",
    "\n",
    "# scale X and y\n",
    "X_scaled = scaler.transform(X_raw.reshape(-1,1)).reshape(-1, SEQ_LEN, 1)\n",
    "y_scaled = scaler.transform(y_raw.reshape(-1,1)).reshape(-1,1)\n",
    "\n",
    "# split train/test\n",
    "split_idx = int(0.9 * len(X_scaled))\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "# -------------- LSTM Model (shared predictor) --------------\n",
    "def make_lstm(seq_len):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(seq_len,1), return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm = make_lstm(SEQ_LEN)\n",
    "print(\"Training LSTM predictor...\")\n",
    "lstm.fit(X_train, y_train, epochs=LSTM_EPOCHS, batch_size=64, validation_split=0.1, verbose=1)\n",
    "pred_test = lstm.predict(X_test)\n",
    "mse = mean_squared_error(y_test, pred_test)\n",
    "print(f\"LSTM scaled MSE on test: {mse:.6f}\")\n",
    "\n",
    "# helper wrapper for predicting next position given last seq of raw pos values:\n",
    "def predict_next_pos(raw_seq):  # raw_seq: array shape (seq_len,)\n",
    "    x = np.array(raw_seq).reshape(-1,1)\n",
    "    x_scaled = scaler.transform(x).reshape(1, SEQ_LEN, 1)\n",
    "    p_scaled = lstm.predict(x_scaled, verbose=0)[0,0]\n",
    "    p = scaler.inverse_transform(np.array([[p_scaled]]))[0,0]\n",
    "    return float(p)\n",
    "\n",
    "\n",
    "# -------------- Gym Environment that uses the LSTM predictor --------------\n",
    "class V2VWithPredictorEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    State:\n",
    "      - VU true current position (scalar)\n",
    "      - Predicted next positions for each CoV (NUM_COV scalars)\n",
    "      - Transmission delays for each CoV (NUM_COV scalars)\n",
    "    Action:\n",
    "      - Discrete: choose one CoV index to request (0..NUM_COV-1)\n",
    "    Reward:\n",
    "      - perception_gain (computed using true next pos) - alpha * delay\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, traces, vu_id, cov_ids, seq_len=SEQ_LEN, fixed_bandwidth=True):\n",
    "        super().__init__()\n",
    "        self.traces = traces\n",
    "        self.vu_id = vu_id\n",
    "        self.cov_ids = cov_ids\n",
    "        self.num_cov = len(cov_ids)\n",
    "        self.seq_len = seq_len\n",
    "        self.t = seq_len\n",
    "        self.max_t = len(next(iter(traces.values()))) - 2\n",
    "        self.observation_space = spaces.Box(low=-1e6, high=1e6, shape=(1 + 2*self.num_cov,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(self.num_cov)\n",
    "        self.base_delays = np.random.uniform(1.0, 3.0, size=self.num_cov)\n",
    "        self.alpha = 0.08\n",
    "        # seed storage\n",
    "        self._seed = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Gym expects env.seed(seed). SB3 calls this on vectorized env creation.\n",
    "        \"\"\"\n",
    "        self._seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            try:\n",
    "                self.observation_space.seed(seed)\n",
    "                self.action_space.seed(seed)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset environment to start a new episode.\n",
    "        Returns: obs, info  (Gym >=0.22 style)\n",
    "        \"\"\"\n",
    "        # let gym handle RNG base if it provides seed via super (some gym versions)\n",
    "        try:\n",
    "            super().reset(seed=seed)\n",
    "        except Exception:\n",
    "            # older gym may not have super().reset(seed=...)\n",
    "            pass\n",
    "\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # pick random start index in valid range\n",
    "        low = self.seq_len\n",
    "        high = max(low + 1, self.max_t - 100)\n",
    "        self.t = np.random.randint(low, high)\n",
    "\n",
    "        # reset delays\n",
    "        self.base_delays = np.random.uniform(1.0, 3.0, size=self.num_cov)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        vu_pos = float(self.traces[self.vu_id][self.t])\n",
    "        predicted = []\n",
    "        for cid in self.cov_ids:\n",
    "            raw_seq = self.traces[cid][self.t - self.seq_len: self.t]\n",
    "            p = predict_next_pos(raw_seq)\n",
    "            predicted.append(p)\n",
    "        delays = self.base_delays.copy()\n",
    "        obs = np.concatenate(([vu_pos], np.array(predicted), np.array(delays))).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns (obs, reward, terminated, truncated, info) to be compatible with new gym.\n",
    "        \"\"\"\n",
    "        assert 0 <= action < self.num_cov\n",
    "\n",
    "        true_next_positions = [float(self.traces[cid][self.t+1]) for cid in self.cov_ids]\n",
    "        vu_true_next = float(self.traces[self.vu_id][self.t+1])\n",
    "\n",
    "        chosen_true_pos = true_next_positions[action]\n",
    "        perception_gain = 1.0 / (1.0 + abs(vu_true_next - chosen_true_pos))\n",
    "\n",
    "        delay = float(self.base_delays[action])\n",
    "        reward = perception_gain - self.alpha * delay\n",
    "\n",
    "        self.t += 1\n",
    "        terminated = self.t >= self.max_t\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {'perception_gain': perception_gain, 'delay': delay}\n",
    "        return obs, float(reward), terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"t={self.t}\")\n",
    "\n",
    "\n",
    "# -------------- Create env and train PPO --------------\n",
    "env = V2VWithPredictorEnv(traces, vu_id, cov_ids, seq_len=SEQ_LEN)\n",
    "# seed the env explicitly (SB3 will also call env.reset(seed=...) internally)\n",
    "env.seed(RANDOM_SEED)\n",
    "\n",
    "# SB3 accepts gym.Env; create model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, seed=RANDOM_SEED)\n",
    "print(\"Training PPO agent...\")\n",
    "model.learn(total_timesteps=PPO_TIMESTEPS)\n",
    "\n",
    "# -------------- Test the learned policy --------------\n",
    "print(\"\\n--- Testing learned policy for a few episodes ---\")\n",
    "for ep in range(3):\n",
    "    obs, info = env.reset()\n",
    "    ep_reward = 0.0\n",
    "    for step in range(50):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # model.predict expects obs (not obs,info). env.step returns 5-tuple.\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        if (step % 10) == 0:\n",
    "            print(f\"step {step:02d} action {action} reward {reward:.3f} info {info}\")\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    print(f\"Episode {ep} total reward: {ep_reward:.3f}\")\n",
    "\n",
    "# -------------- Integration notes --------------\n",
    "\"\"\"\n",
    "To run on real SUMO traces:\n",
    "- Export per-vehicle position sequences and build `traces` dict: {'veh_0': np.array([...]), ...}\n",
    "- Ensure veh order: choose one as VU and NUM_COV CoVs (cov_ids).\n",
    "- Train LSTM with SUMO sliding windows; or save/load the trained lstm:\n",
    "    lstm.save('lstm_predictor.h5')\n",
    "    lstm = tf.keras.models.load_model('lstm_predictor.h5')\n",
    "- For faster experiments reduce PPO_TIMESTEPS (e.g., 512) to sanity check training.\n",
    "\n",
    "Notes:\n",
    "- This env.reset / env.step implementation returns/handles the newer gym API conventions.\n",
    "- If you plan to vectorize the env, consider building separate predictor copies per worker to avoid thread-safety issues.\n",
    "\"\"\"\n",
    "print(\"\\nDone. LSTM + PPO integrated pipeline (Gym-compatible reset/step) ready — replace synthetic traces with SUMO traces in `traces` to run on real data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f493861",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
